{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89647bd-0aee-43a3-85f2-23e155de4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries need for doing the job\n",
    "import requests\n",
    "import numpy\n",
    "import re\n",
    "import binascii\n",
    "import itertools\n",
    "from time import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d9d1a6-39f7-47d8-bc77-b6c6c470d05c",
   "metadata": {},
   "source": [
    "##### create class to organize methods used for the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c991e550-bf58-4c7d-886c-e68cca431848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BdaTools:\n",
    "    #Read file from remote and save the content in local file\n",
    "    def download_remote_file(self, url):\n",
    "        # local file name\n",
    "        local_filename = url.split('/')[-1]\n",
    "\n",
    "        # get file from remote and write the contet to local file\n",
    "        with requests.get(url, stream=True) as req:\n",
    "            with open(local_filename, 'wb') as file:\n",
    "                for content in req.iter_content(chunk_size=8192):\n",
    "                    file.write(content)\n",
    "\n",
    "        return local_filename\n",
    "    \n",
    "    def read_lyrics(self, filename):\n",
    "        \"\"\" \n",
    "        Reads txt file and returns tuple with\n",
    "        list of top 5,000 words and \n",
    "        the index : frequency for each track\n",
    "        \"\"\"\n",
    "        content = [] # list with word index : word count for each track\n",
    "        string = '%'\n",
    "        find = False \n",
    "        words = [] \n",
    "        track_id = [] # list with track ID's from the MSD\n",
    "        mxm_tid = [] # track ID's from musiXmatch\n",
    "        str_data = []\n",
    "\n",
    "        read_file = open(filename, \"r\")\n",
    "\n",
    "        for line in read_file:\n",
    "            if find:\n",
    "                line = line.strip() \n",
    "                index1 = line.find(',') # finds index of 1st comma\n",
    "                index2 = line.find(',', index1+1) # finds index of 2nd comma\n",
    "                track_id.append(line[:index1]) # appends track id to list \n",
    "                mxm_tid.append(line[:index2]) \n",
    "                res = '{' + line[index2+1:] + '}' \n",
    "                d = eval(res) # converts string to actual dictionary \n",
    "                content.append(d) # appends track data to content list\n",
    "            else:\n",
    "                # obtaining line with 5,000 words \n",
    "                if line.startswith(string):\n",
    "                    line = line[1:] # getting rid of %\n",
    "                    words = [word.strip() for word in line.split(',')]\n",
    "                    find = True # already found list of words \n",
    "        read_file.close() \n",
    "\n",
    "\n",
    "        return (words, content, track_id, mxm_tid)\n",
    "    \n",
    "    def create_vectors(list_dict, num_words):\n",
    "    \"\"\"\n",
    "    Returns a list x for all the data points. \n",
    "    \n",
    "    Each element of x is a NumPy vector with 5,000 elements, \n",
    "    one for each word.\n",
    "    \"\"\"\n",
    "    x = [] \n",
    "    for d in list_dict:\n",
    "        # initializing numpy vector containing 5,000 (number of words) zeros\n",
    "        temp = np.zeros(num_words, dtype=np.float64)\n",
    "        for key, val in d.items():\n",
    "            key -= 1 # indexing in data starts at 1\n",
    "            temp[key] = 1 # adding word and its frequency to vector \n",
    "\n",
    "        x.append(temp) # appends vector to x  \n",
    "\n",
    "    return x\n",
    "    \n",
    "# organize methods for similarity analysis\n",
    "class SimilarityTools:\n",
    "    \n",
    "    maxShingleID = 2**32-1  # record the maximum shingle ID that we assigned\n",
    "    nextPrime = 4294967311  # next prime number after maxShingleID\n",
    "    # hash tables (multihash)\n",
    "    max_hash1 = 5 * 1000000 -673\n",
    "    max_hash2 = 5 * 1000000 +673\n",
    "    \n",
    "    # generate random prime number\n",
    "    def generate_prime(self,divisor=1,nsig=None, bands=None, rows=None):\n",
    "        if not bands and not rows:\n",
    "            return numpy.random.randint(0, self.nextPrime/divisor, size=(nsig,),dtype=numpy.int64)\n",
    "        elif not rows:\n",
    "            return numpy.random.randint(0, self.nextPrime/divisor, size=(bands, ))\n",
    "        else:\n",
    "            return numpy.random.randint(0, self.nextPrime/divisor, size=(bands, rows))\n",
    "    \n",
    "    # get shingles from lyrics\n",
    "    def get_shingles(self, lyrics, k=5):\n",
    "        L = len(lyrics)\n",
    "        shingles = set()\n",
    "        for i in range(L-k+1):\n",
    "            shingle = lyrics[i:i+k]\n",
    "            crc = binascii.crc32(shingle.encode('utf-8')) \n",
    "            shingles.add(crc)\n",
    "        return shingles\n",
    "    \n",
    "    # shingle vectors \n",
    "    def get_shingles_vectors(self, lyricss,sample=100,k=5):\n",
    "        shingles_vectors = []\n",
    "        for lyrics in lyricss[:sample]:\n",
    "            sh = list(self.get_shingles(lyrics, k=k))\n",
    "            shingles_vectors.append(sh)\n",
    "        return shingles_vectors\n",
    "    \n",
    "    # jaccard_similarity_score\n",
    "    def jaccard_similarity_score(self, x, y):\n",
    "        \"\"\"\n",
    "        Jaccard Similarity J (A,B) = | Intersection (A,B) | /\n",
    "                                        | Union (A,B) |\n",
    "        \"\"\"\n",
    "        intersection_cardinality = len(set(x).intersection(set(y)))\n",
    "        union_cardinality = len(set(x).union(set(y)))\n",
    "        return intersection_cardinality / float(union_cardinality)\n",
    "    \n",
    "    # vectorized min-hashing\n",
    "    def minhash_vectorized(self, shingles, A, B, nextPrime, maxShingleID, nsig):\n",
    "        signature = numpy.ones((nsig,)) * (maxShingleID + 1)\n",
    "\n",
    "        for ShingleID in shingles:\n",
    "            hashCodes = ((A*ShingleID + B) % nextPrime) % maxShingleID\n",
    "            numpy.minimum(signature, hashCodes, out=signature)\n",
    "\n",
    "        return signature\n",
    "    \n",
    "    # minhash candidates\n",
    "    def get_minhash_candidates(self, domain,A, B, nextPrime, maxShingleID, k=3, s=0.5, nsig=50):\n",
    "        signatures = []  # signatures for all files\n",
    "        for lyrics in domain:\n",
    "            shingles = self.get_shingles(lyrics, k=k)\n",
    "            signature = self.minhash_vectorized(shingles, A, B, nextPrime, maxShingleID, nsig)\n",
    "            signatures.append(signature)\n",
    "\n",
    "        s = s  # similarity threshold\n",
    "        Nfiles = len(signatures)\n",
    "        candidates = []\n",
    "        for i in range(Nfiles):\n",
    "            for j in range(i+1, Nfiles):\n",
    "                Jsim = numpy.mean(signatures[i] == signatures[j])  # average number of similar items in \n",
    "                if Jsim >= s:                                      # two vectors, equivalente to Jaccard \n",
    "                    candidates.append((i,j))\n",
    "        return candidates\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d09422ad-3bf5-47f0-aced-3810d12e6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of the class and make it ready for use\n",
    "tools = BdaTools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ddcd50-991a-46fc-bb9d-8e31b5ac9a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file name is , mxm_dataset_test.txt\n",
      "Trian file name is , mxm_dataset_train.txt\n"
     ]
    }
   ],
   "source": [
    "#download file and check file name\n",
    "musiXmatch_test_data_url = \"https://people.arcada.fi/~fentawaw/mxm_dataset_test.txt\"\n",
    "musiXmatch_train_data_url = \"https://people.arcada.fi/~fentawaw/mxm_dataset_train.txt\"\n",
    "test_file_name=tools.download_remote_file(musiXmatch_test_data_url)\n",
    "train_file_name=tools.download_remote_file(musiXmatch_train_data_url)\n",
    "# check downloaded file\n",
    "print(f'Test file name is , {test_file_name}')\n",
    "print(f'Trian file name is , {train_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ec71c77-4c05-4669-9cfb-787447847446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample from test\n",
      "sample words : ['i', 'the', 'you', 'to', 'and', 'a', 'me', 'it', 'not', 'in', 'my', 'is', 'of', 'your', 'that', 'do', 'on', 'are', 'we', 'am', 'will', 'all', 'for', 'no', 'be', 'have', 'love', 'so', 'know', 'this', 'but', 'with', 'what', 'just', 'when', 'like', 'now', 'que', 'time', 'can', 'come', 'de', 'there', 'go', 'up', 'oh', 'la', 'one', 'they', 'out']\n",
      "sample content : [{2: 19, 4: 7, 5: 6, 10: 1, 12: 13, 13: 6, 17: 4, 18: 6, 22: 1, 23: 1, 30: 11, 32: 4, 33: 6, 46: 8, 60: 1, 73: 1, 82: 1, 89: 1, 103: 5, 116: 1, 118: 5, 134: 1, 162: 1, 184: 1, 201: 3, 212: 5, 234: 5, 260: 3, 268: 4, 274: 4, 275: 1, 279: 4, 297: 1, 351: 6, 404: 9, 449: 4, 462: 1, 484: 4, 517: 6, 521: 5, 730: 5, 814: 1, 878: 1, 1003: 10, 1133: 5, 1649: 7, 2090: 5, 2258: 1, 2358: 1, 2740: 4, 3016: 1, 3024: 1, 3270: 7, 3741: 9, 4435: 4}, {1: 79, 2: 66, 3: 15, 4: 7, 5: 8, 6: 9, 7: 5, 8: 5, 9: 4, 10: 57, 11: 5, 12: 4, 13: 2, 14: 3, 15: 2, 17: 1, 18: 6, 19: 1, 20: 56, 21: 4, 22: 3, 23: 2, 24: 1, 25: 5, 28: 6, 29: 3, 30: 2, 31: 2, 32: 5, 33: 6, 34: 4, 35: 3, 36: 4, 37: 2, 41: 2, 43: 2, 44: 3, 45: 4, 48: 1, 49: 2, 50: 1, 51: 2, 52: 5, 54: 2, 57: 2, 58: 3, 59: 1, 62: 1, 63: 2, 64: 1, 68: 1, 70: 1, 76: 1, 78: 8, 81: 1, 94: 2, 95: 3, 101: 1, 105: 1, 106: 1, 109: 3, 115: 3, 122: 2, 123: 1, 126: 1, 133: 1, 134: 1, 135: 1, 136: 1, 144: 1, 157: 1, 168: 1, 190: 2, 195: 1, 203: 1, 207: 2, 212: 1, 215: 1, 216: 3, 223: 1, 228: 1, 229: 1, 230: 1, 234: 1, 239: 1, 241: 2, 242: 1, 251: 1, 252: 1, 258: 1, 287: 1, 302: 1, 304: 2, 310: 1, 318: 1, 326: 2, 337: 1, 338: 1, 339: 2, 358: 1, 373: 1, 378: 7, 384: 2, 389: 2, 393: 1, 456: 1, 461: 1, 464: 1, 465: 1, 470: 2, 486: 51, 521: 2, 548: 1, 555: 2, 563: 1, 564: 1, 601: 1, 631: 1, 637: 1, 661: 1, 707: 1, 748: 1, 772: 1, 779: 1, 787: 1, 793: 2, 876: 1, 880: 1, 890: 1, 1036: 1, 1037: 2, 1080: 1, 1144: 1, 1146: 1, 1180: 2, 1192: 1, 1197: 1, 1214: 1, 1260: 1, 1267: 1, 1341: 1, 1366: 1, 1368: 1, 1381: 1, 1463: 1, 1466: 1, 1520: 1, 1665: 1, 1686: 1, 1848: 1, 2011: 1, 2019: 1, 2063: 1, 2087: 2, 2189: 1, 2255: 1, 2284: 1, 2321: 1, 2397: 1, 2691: 1, 2757: 1, 3012: 1, 3032: 1, 3179: 1, 3339: 1, 3609: 1, 3627: 1, 3745: 1, 4335: 1, 4457: 1, 4600: 1, 4632: 1}]\n",
      "sample track_id : ['TRAABRX12903CC4816', 'TRAADFO128F92E1E91']\n",
      "sample mxm_tid : ['TRAABRX12903CC4816,1548880', 'TRAADFO128F92E1E91,5325944']\n"
     ]
    }
   ],
   "source": [
    "# read lyrices dat for test\n",
    "words, content, track_id, mxm_tid = tools.read_lyrics(test_file_name)\n",
    "# print sample data \n",
    "print(f'sample from test')\n",
    "print(f'sample words : {words[:50]}')\n",
    "print(f'sample content : {content[:2]}')\n",
    "print(f'sample track_id : {track_id[:2]}')\n",
    "print(f'sample mxm_tid : {mxm_tid[:2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b274c03f-75ca-4969-a861-921925be29d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample from train\n",
      "sample words : ['i', 'the', 'you', 'to', 'and', 'a', 'me', 'it', 'not', 'in', 'my', 'is', 'of', 'your', 'that', 'do', 'on', 'are', 'we', 'am', 'will', 'all', 'for', 'no', 'be', 'have', 'love', 'so', 'know', 'this', 'but', 'with', 'what', 'just', 'when', 'like', 'now', 'que', 'time', 'can', 'come', 'de', 'there', 'go', 'up', 'oh', 'la', 'one', 'they', 'out']\n",
      "sample content : [{1: 6, 2: 4, 3: 2, 4: 2, 5: 5, 6: 3, 7: 1, 8: 1, 11: 1, 12: 2, 13: 3, 14: 1, 15: 1, 18: 2, 19: 2, 20: 2, 21: 2, 23: 4, 25: 1, 26: 2, 28: 1, 30: 1, 36: 2, 42: 1, 45: 1, 54: 2, 56: 1, 57: 1, 68: 1, 99: 1, 192: 2, 249: 1, 264: 1, 356: 1, 389: 1, 561: 1, 639: 1, 656: 1, 687: 1, 761: 1, 773: 1, 804: 1, 869: 2, 914: 1, 1035: 1, 1156: 1, 1221: 1, 1287: 1, 1364: 1, 1407: 1, 1533: 2, 1857: 1, 2096: 1, 2117: 1, 2482: 2, 2548: 1, 2705: 1, 2723: 1, 2868: 2, 2992: 2, 3455: 1, 3717: 1, 3851: 1, 4322: 1, 4382: 1, 4613: 1, 4713: 1, 4906: 1}, {1: 10, 3: 17, 4: 8, 5: 2, 6: 2, 7: 1, 8: 3, 9: 2, 10: 3, 11: 4, 12: 3, 14: 7, 15: 5, 16: 5, 18: 6, 23: 4, 24: 1, 26: 6, 28: 2, 29: 5, 31: 3, 33: 3, 35: 2, 39: 3, 40: 1, 43: 5, 47: 7, 52: 2, 57: 3, 58: 2, 61: 2, 62: 2, 68: 2, 71: 4, 74: 2, 76: 4, 81: 5, 84: 2, 86: 3, 87: 2, 88: 2, 89: 2, 92: 2, 101: 1, 107: 1, 111: 2, 113: 1, 118: 3, 119: 2, 130: 3, 131: 3, 165: 1, 168: 1, 169: 2, 178: 4, 180: 2, 188: 2, 196: 7, 200: 1, 219: 2, 229: 2, 256: 1, 279: 2, 349: 4, 384: 1, 393: 2, 424: 2, 472: 3, 589: 1, 843: 2, 1038: 2, 1351: 1, 1542: 2, 2437: 2}]\n",
      "sample track_id : ['TRAAAAV128F421A322', 'TRAAABD128F429CF47']\n",
      "sample mxm_tid : ['TRAAAAV128F421A322,4623710', 'TRAAABD128F429CF47,6477168']\n"
     ]
    }
   ],
   "source": [
    "# read lyrices dat for train \n",
    "_words, _content, _track_id, _mxm_tid = tools.read_lyrics(train_file_name)\n",
    "\n",
    "# print sample data \n",
    "print(f'sample from train')\n",
    "print(f'sample words : {_words[:50]}')\n",
    "print(f'sample content : {_content[:2]}')\n",
    "print(f'sample track_id : {_track_id[:2]}')\n",
    "print(f'sample mxm_tid : {_mxm_tid[:2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf0eeea-0c51-4950-8f04-e569da8024ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
